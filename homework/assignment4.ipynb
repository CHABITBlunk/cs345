{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CS345 Fall 2024 Assignment 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.linear_model import Ridge \n",
    "from sklearn.linear_model import ridge_regression \n",
    "import io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1:  Linear regression on medical insurance data\n",
    "\n",
    "### The data\n",
    "\n",
    "We will work on predicting a person's insurance charges based on a collection of features that include age, gender, body-mass-index (BMI), number of children, whether a person smokes, and the region in the US they live in.\n",
    "Since the file contains string data in addition to numeric data let's first read the file using the Python `requests` module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "# if you don't have requests installed you can use urlopen which is part\n",
    "# of the python standard library\n",
    "# from urllib.request import urlopen\n",
    "link = \"https://github.com/asabenhur/CS345/raw/master/fall22/data/insurance.csv\"\n",
    "# retrieve the contents of the file\n",
    "contents = requests.get(link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the first five rows of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['age,sex,bmi,children,smoker,region,charges',\n",
       " '19,female,27.9,0,yes,southwest,16884.924',\n",
       " '18,male,33.77,1,no,southeast,1725.5523',\n",
       " '28,male,33,3,no,southeast,4449.462',\n",
       " '33,male,22.705,0,no,northwest,21984.47061']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contents.text.split()[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the header specifies the names of the features:\n",
    "\n",
    "```\n",
    "age,sex,bmi,children,smoker,region,charges\n",
    "```\n",
    "\n",
    "The names are self explanatory.  We will use the last column as our label/target variable, and try to predict it from the other variables.\n",
    "\n",
    "Since the data is a mix of numerical and categorical variables, we will need to do some work to create a feature matrix from this file.\n",
    "\n",
    "This data has three types of features:\n",
    "\n",
    "* Numerical data (the age, bmi, and children features)\n",
    "* Categorical data that has one of two values (the sex and smoker features)\n",
    "* Categorical data that has more than two possible values.  The \"region\" feature has the values \"southwest\", \"southeast\", \"northwest\", and \"northeast\".\n",
    "\n",
    "Numerical data can be left as is (but will need to be standardized); binary categorical data can be converted to 0/1.  Categorical variables with more than two possible values require a different approach, which is called \"one-hot-encoding\", where each value receives its own feature, and the single categorical variable is replaced with a set of features.  The number of features equals the number of unique values that the categorical variable takes, and a categorical value is encoded as a vector of zeros, with a single value of one in the feature that corresponds to the given value.  Let us demonstrate this using the one-hot-encoder of scikit-learn on our \"region\" variable which has four possible values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 1.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# a mock dataset of regions:\n",
    "X = [['southwest'], ['southeast'], ['northwest'], ['northeast'], ['southeast']]\n",
    "# create an instance of one hot encoder\n",
    "encoder = OneHotEncoder()\n",
    "# apply it to the data:\n",
    "encoder.fit_transform(X).toarray()\n",
    "# we have converted the output of fit_transform from a sparse array\n",
    "# type to a regular NumPy array so we can easily see the values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the single variable was represented by four features, one for each possible value.\n",
    "\n",
    "**Note:**  In your code you may not use the scikit-learn one-hot-encoder.\n",
    "\n",
    "You might ask why not simply encode the four region values using the numbers 0-3.  The issue with doing that is that this representation imposes an order on the values, an order that does not necessarily reflect the relationship between these values in the real world."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your tasks\n",
    "\n",
    "Your tasks are as follows:\n",
    "\n",
    "* Create a feature matrix and labels vector from the data as described above.\n",
    "* Split the data into separate train and test sets with 30% of the data reserved for testing.\n",
    "* Evaluate the error of linear regression on the train and test sets.  In doing so, first standardize the features.  Note that not all the features should be standardized - only the age and BMI features need that, because their values are significantly larger than one.  Note that when standardizing, *only the features need to be standardized*.  The labels should remain unaltered.  This dataset can lead to a linear regression solution that suffers from numerical instabilities.  To address that, use the scikit-learn implementation of ridge regression with a very small value of the regularization parameter $\\alpha$.\n",
    "* As discussed in class, the magnitude of the components of the weight vector provide an indication of the usefulness of a feature. Based on the magnitude of the components of the weight vector, choose a subset of the features that are most useful for the regression task (note that large negative values also suggest usefulness).  Compare the error before and after removing the features with the lowest importance based on the weight vector.  Comment on your results.  Were the region features chosen?  If not, hypothesize why that is the case.\n",
    "* As a baseline, compute the error of a naive regression method that for a given regression problem always returns the mean label value.  Compare this error with the error you got using linear regression and comment on the result.  Why is computing the error of this naive method a useful thing to do?\n",
    "\n",
    "For evaluating regression error use mean asbsolute error (MAE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mae for all features: 4079.2602\n",
      "mae for selected features: 5515.4269\n",
      "selected region features: ['region_southeast', 'region_southwest', 'region_northwest', 'region_northeast']\n",
      "naive mae: 9167.9570\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(link)\n",
    "df.loc[df['sex'] == 'female', 'sex'] = 0\n",
    "df.loc[df['sex'] == 'male', 'sex'] = 1\n",
    "df.loc[df['smoker'] == 'no', 'smoker'] = 0\n",
    "df.loc[df['smoker'] == 'yes', 'smoker'] = 1\n",
    "y = df['charges']\n",
    "df = df.drop(columns=['charges'])\n",
    "df = pd.get_dummies(df, columns=['region'], dtype=float)\n",
    "X = df\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "X_test['age'] = (X_test['age'] - X_test['age'].mean()) / X_test['age'].std()\n",
    "X_test['bmi'] = (X_test['bmi'] - X_test['bmi'].mean()) / X_test['bmi'].std()\n",
    "X_train['age'] = (X_train['age'] - X_train['age'].mean()) / X_train['age'].std()\n",
    "X_train['bmi'] = (X_train['bmi'] - X_train['bmi'].mean()) / X_train['bmi'].std()\n",
    "\n",
    "weights = ridge_regression(X, y, alpha=0.00001)\n",
    "\n",
    "feature_names = X.columns\n",
    "abs_weights = np.abs(weights)\n",
    "sorted_indices = np.argsort(abs_weights)[::-1]\n",
    "sorted_features = feature_names[sorted_indices]\n",
    "important_features = sorted_features[:5]\n",
    "\n",
    "X_train_selected = X_train[important_features]\n",
    "X_test_selected = X_test[important_features]\n",
    "\n",
    "ridge = Ridge(alpha=0.00001)\n",
    "ridge.fit(X_train_selected, y_train)\n",
    "y_pred_selected = ridge.predict(X_test_selected)\n",
    "\n",
    "ridge_full = Ridge(alpha=0.00001)\n",
    "ridge_full.fit(X_train, y_train)\n",
    "y_pred_full = ridge_full.predict(X_test)\n",
    "\n",
    "mae_full = mean_absolute_error(y_test, y_pred_full)\n",
    "mae_selected = mean_absolute_error(y_test, y_pred_selected)\n",
    "\n",
    "print(f\"mae for all features: {mae_full:.4f}\")\n",
    "print(f\"mae for selected features: {mae_selected:.4f}\")\n",
    "\n",
    "region_features = [col for col in feature_names if 'region' in col]\n",
    "selected_regions = [feat for feat in important_features if feat in region_features]\n",
    "\n",
    "print(f\"selected region features: {selected_regions}\")\n",
    "\n",
    "\n",
    "y_pred_naive = np.full_like(y_test, y_train.mean())\n",
    "\n",
    "mae_naive = mean_absolute_error(y_test, y_pred_naive)\n",
    "print(f'naive mae: {mae_naive:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, the MAE after selecting features based on relevance was higher. Regions were included, as they had bigger weights than other factors like age, sex, bmi, or children.\n",
    "\n",
    "The ridge regression MAE is much lower than the naive MAE, which suggests that the regression is successfully capturing relationships in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2:  polynomial regression\n",
    "\n",
    "In this part we will explore polynomial regression using the [California housing dataset](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_california_housing.html#sklearn.datasets.fetch_california_housing), which is distributed with scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "data = fetch_california_housing()\n",
    "X = data.data\n",
    "y = data.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A complete description of the dataset and its features is contained in the `DESCR` attribute of the dataset:\n",
    "\n",
    "```python\n",
    "print(data.DESCR)\n",
    "```\n",
    "\n",
    "Let's plot a histogram of the labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([   9.,   14.,   61.,  255.,  538.,  676.,  546.,  803.,  881.,\n",
       "         706.,  812.,  715.,  836.,  815.,  921.,  960.,  786.,  817.,\n",
       "         703.,  505.,  620.,  661.,  578.,  508.,  432.,  439.,  463.,\n",
       "         331.,  320.,  216.,  224.,  232.,  245.,  252.,  286.,  193.,\n",
       "         160.,  149.,  117.,  133.,  104.,   99.,   80.,   90.,  106.,\n",
       "          66.,   49.,   51.,   47., 1030.]),\n",
       " array([0.14999  , 0.2469904, 0.3439908, 0.4409912, 0.5379916, 0.634992 ,\n",
       "        0.7319924, 0.8289928, 0.9259932, 1.0229936, 1.119994 , 1.2169944,\n",
       "        1.3139948, 1.4109952, 1.5079956, 1.604996 , 1.7019964, 1.7989968,\n",
       "        1.8959972, 1.9929976, 2.089998 , 2.1869984, 2.2839988, 2.3809992,\n",
       "        2.4779996, 2.575    , 2.6720004, 2.7690008, 2.8660012, 2.9630016,\n",
       "        3.060002 , 3.1570024, 3.2540028, 3.3510032, 3.4480036, 3.545004 ,\n",
       "        3.6420044, 3.7390048, 3.8360052, 3.9330056, 4.030006 , 4.1270064,\n",
       "        4.2240068, 4.3210072, 4.4180076, 4.515008 , 4.6120084, 4.7090088,\n",
       "        4.8060092, 4.9030096, 5.00001  ]),\n",
       " <BarContainer object of 50 artists>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAFfCAYAAAC1P4ylAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjEsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvc2/+5QAAAAlwSFlzAAAPYQAAD2EBqD+naQAAHH5JREFUeJzt3X1wVPXZ//HPNg9LSJOVBNllh6CxTX1KtBhsmmiFNiGU8iDDTKOFUjoyLQ4Q3QJFU9qan6OJpCOkNZWKZQChafyjRp25K02oNjSD1BBNhWh9GKOGmjXtNG4STDcYzv0Ht2d+SwgKnmWTb96vmTPjnr129zqMfrz47jlnXZZlWQIAGOVzsW4AAOA8wh0ADES4A4CBCHcAMBDhDgAGItwBwECEOwAYKD7WDUTLyZMn9d577yklJUUulyvW7QDAZ2ZZlvr6+uT3+/W5z519Njc23N977z1lZGTEug0AcFxnZ6emTZt21hpjwz0lJUXSqT+E1NTUGHcDAJ9db2+vMjIy7Hw7G2PD/eOlmNTUVMIdgFE+zVIzX6gCgIEIdwAwEOEOAAYi3AHAQIQ7ABiIcAcAAxHuAGAgwh0ADES4A4CBCHcAMBDhDgAGItwBwEDG3jgMAEaDS+/+nzPuf/uB+VH9XCZ3ADAQ4Q4ABiLcAcBAhDsAGOicw/3AgQNauHCh/H6/XC6XnnzyyYjnLctSeXm5/H6/kpKSNHv2bLW3t0fUhMNhlZaWavLkyUpOTtaiRYt07NixiJqenh4tX75cHo9HHo9Hy5cv1wcffHDOBwgA49E5h/vx48d17bXXqqam5ozPV1VVacuWLaqpqVFLS4t8Pp/mzJmjvr4+uyYQCKi+vl51dXVqbm5Wf3+/FixYoKGhIbtm6dKlamtr0759+7Rv3z61tbVp+fLl53GIADD+uCzLss77xS6X6uvrtXjxYkmnpna/369AIKC77rpL0qkp3ev1avPmzVq1apVCoZAuvvhi7dmzR7fccosk6b333lNGRob++Mc/au7cuXr11Vd11VVX6dChQ8rLy5MkHTp0SPn5+frHP/6hyy+/fFgv4XBY4XDYfvzxD8mGQiF+QxVAzDh5KmRvb688Hs+nyjVH19w7OjoUDAZVXFxs73O73Zo1a5YOHjwoSWptbdWJEyciavx+v7Kzs+2a559/Xh6Pxw52SfrqV78qj8dj15yusrLSXsLxeDzKyMhw8tAAYExxNNyDwaAkyev1Ruz3er32c8FgUImJiZo0adJZa6ZMmTLs/adMmWLXnK6srEyhUMjeOjs7P/PxAMBYFZUrVF0uV8Rjy7KG7Tvd6TVnqj/b+7jdbrnd7vPoFgDM4+jk7vP5JGnYdN3d3W1P8z6fT4ODg+rp6Tlrzfvvvz/s/f/1r38N+1sBAGA4R8M9MzNTPp9PjY2N9r7BwUE1NTWpoKBAkpSbm6uEhISImq6uLh09etSuyc/PVygU0gsvvGDX/O1vf1MoFLJrAAAjO+dlmf7+fr355pv2446ODrW1tSktLU3Tp09XIBBQRUWFsrKylJWVpYqKCk2cOFFLly6VJHk8Hq1cuVLr169Xenq60tLStGHDBuXk5KioqEiSdOWVV+qb3/ymfvCDH+iRRx6RJP3whz/UggULznimDAAg0jmH++HDh/X1r3/dfrxu3TpJ0ooVK7Rr1y5t3LhRAwMDWr16tXp6epSXl6eGhgalpKTYr9m6davi4+NVUlKigYEBFRYWateuXYqLi7Nrfve73+mOO+6wz6pZtGjRiOfWAwAifabz3EezczkfFACixYjz3AEAowPhDgAGItwBwECEOwAYiN9QxQVxpi+Vov0bksB4xuQOAAYi3AHAQIQ7ABiIcAcAAxHuAGAgwh0ADES4A4CBCHcAMBDhDgAGItwBwEDcfgCOGune1QAuLCZ3ADAQ4Q4ABiLcAcBAhDsAGIhwBwADEe4AYCDCHQAMRLgDgIEIdwAwEOEOAAbi9gOGGOmy/7cfmH+BOwEwGjC5A4CBCHcAMBDhDgAGItwBwECEOwAYiHAHAAMR7gBgIMIdAAxEuAOAgQh3ADAQ4Q4ABiLcAcBA3DgMn4ibkgFjD5M7ABiIcAcAAxHuAGAgwh0ADES4A4CBOFsG522ks2ii9b6cnQN8eo5P7h999JF++tOfKjMzU0lJSbrssst077336uTJk3aNZVkqLy+X3+9XUlKSZs+erfb29oj3CYfDKi0t1eTJk5WcnKxFixbp2LFjTrc7bl169/8M2wCYw/Fw37x5s37zm9+opqZGr776qqqqqvSLX/xCDz30kF1TVVWlLVu2qKamRi0tLfL5fJozZ476+vrsmkAgoPr6etXV1am5uVn9/f1asGCBhoaGnG4ZAIzj+LLM888/r5tvvlnz55/6K/Sll16q3//+9zp8+LCkU1N7dXW1Nm3apCVLlkiSdu/eLa/Xq9raWq1atUqhUEg7duzQnj17VFRUJEnau3evMjIytH//fs2dO3fY54bDYYXDYftxb2+v04cGAGOG45P7jTfeqD//+c96/fXXJUl///vf1dzcrG9961uSpI6ODgWDQRUXF9uvcbvdmjVrlg4ePChJam1t1YkTJyJq/H6/srOz7ZrTVVZWyuPx2FtGRobThzZqsKQC4JM4PrnfddddCoVCuuKKKxQXF6ehoSHdf//9+s53viNJCgaDkiSv1xvxOq/Xq3feeceuSUxM1KRJk4bVfPz605WVlWndunX2497eXqMDHgDOxvFwf/zxx7V3717V1tbq6quvVltbmwKBgPx+v1asWGHXuVyuiNdZljVs3+nOVuN2u+V2uz/7AQCAARwP9x//+Me6++67deutt0qScnJy9M4776iyslIrVqyQz+eTdGo6nzp1qv267u5ue5r3+XwaHBxUT09PxPTe3d2tgoICp1sGAOM4Hu4ffvihPve5yKX8uLg4+1TIzMxM+Xw+NTY2asaMGZKkwcFBNTU1afPmzZKk3NxcJSQkqLGxUSUlJZKkrq4uHT16VFVVVU63jBjhuwIgehwP94ULF+r+++/X9OnTdfXVV+ull17Sli1bdNttt0k6tRwTCARUUVGhrKwsZWVlqaKiQhMnTtTSpUslSR6PRytXrtT69euVnp6utLQ0bdiwQTk5OfbZMwCAkTke7g899JB+9rOfafXq1eru7pbf79eqVav085//3K7ZuHGjBgYGtHr1avX09CgvL08NDQ1KSUmxa7Zu3ar4+HiVlJRoYGBAhYWF2rVrl+Li4pxuGQCM43i4p6SkqLq6WtXV1SPWuFwulZeXq7y8fMSaCRMm6KGHHoq4+AkA8Olw4zAAMBDhDgAGItwBwECEOwAYiHAHAAMR7gBgIMIdAAxEuAOAgQh3ADAQ4Q4ABiLcAcBAhDsAGIhwBwADEe4AYCDCHQAMRLgDgIEIdwAwkOO/xISxix+sBszB5A4ABmJyNxzTODA+Ee4YM870P6q3H5gfg06A0Y9lGQAwEOEOAAYi3AHAQIQ7ABiIcAcAAxHuAGAgwh0ADMR57qPASBcacQ43gPPF5A4ABiLcAcBAhDsAGIg1d4wr3J8G4wWTOwAYiHAHAAMR7gBgIMIdAAxEuAOAgQh3ADAQ4Q4ABiLcAcBAhDsAGIhwBwADEe4AYCDuLQMjjXSPfGC8YHIHAANFJdz/+c9/6rvf/a7S09M1ceJEffnLX1Zra6v9vGVZKi8vl9/vV1JSkmbPnq329vaI9wiHwyotLdXkyZOVnJysRYsW6dixY9FoFwCM43i49/T06IYbblBCQoKeeeYZvfLKK3rwwQd10UUX2TVVVVXasmWLampq1NLSIp/Ppzlz5qivr8+uCQQCqq+vV11dnZqbm9Xf368FCxZoaGjI6ZYBwDiOr7lv3rxZGRkZ2rlzp73v0ksvtf/ZsixVV1dr06ZNWrJkiSRp9+7d8nq9qq2t1apVqxQKhbRjxw7t2bNHRUVFkqS9e/cqIyND+/fv19y5c4d9bjgcVjgcth/39vY6fWgAMGY4Prk//fTTmjlzpr797W9rypQpmjFjhh599FH7+Y6ODgWDQRUXF9v73G63Zs2apYMHD0qSWltbdeLEiYgav9+v7Oxsu+Z0lZWV8ng89paRkeH0oQHAmOF4uL/11lvatm2bsrKy9Kc//Um333677rjjDj322GOSpGAwKEnyer0Rr/N6vfZzwWBQiYmJmjRp0og1pysrK1MoFLK3zs5Opw8NAMYMx5dlTp48qZkzZ6qiokKSNGPGDLW3t2vbtm363ve+Z9e5XK6I11mWNWzf6c5W43a75Xa7P2P3AGAGxyf3qVOn6qqrrorYd+WVV+rdd9+VJPl8PkkaNoF3d3fb07zP59Pg4KB6enpGrAEAjMzxcL/hhhv02muvRex7/fXXdckll0iSMjMz5fP51NjYaD8/ODiopqYmFRQUSJJyc3OVkJAQUdPV1aWjR4/aNQCAkTm+LPOjH/1IBQUFqqioUElJiV544QVt375d27dvl3RqOSYQCKiiokJZWVnKyspSRUWFJk6cqKVLl0qSPB6PVq5cqfXr1ys9PV1paWnasGGDcnJy7LNnAAAjczzcr7/+etXX16usrEz33nuvMjMzVV1drWXLltk1Gzdu1MDAgFavXq2enh7l5eWpoaFBKSkpds3WrVsVHx+vkpISDQwMqLCwULt27VJcXJzTLQOAcVyWZVmxbiIaent75fF4FAqFlJqaGut2zmqk+6C8/cD8c6ofj6L5ZzTSewPn4lz/+z6bc8k17i0DAAYi3AHAQIQ7ABiIcAcAAxHuAGAgwh0ADES4A4CBCHcAMBA/kD2KcbESgPPF5A4ABmJyv8CYxgFcCEzuAGAgwh0ADES4A4CBCHcAMBDhDgAGItwBwECEOwAYiHAHAANxERPGtGheFObkb18CFxqTOwAYiHAHAAMR7gBgIMIdAAxEuAOAgQh3ADAQ4Q4ABiLcAcBAhDsAGIhwBwADEe4AYCDuLQNE0ZnuT8O9aXAhMLkDgIEIdwAwEOEOAAYi3AHAQIQ7ABiIcAcAA3EqJOCAaP7cH3A+mNwBwEBM7sA5YkrHWMDkDgAGItwBwECEOwAYiHAHAAMR7gBgoKiHe2VlpVwulwKBgL3PsiyVl5fL7/crKSlJs2fPVnt7e8TrwuGwSktLNXnyZCUnJ2vRokU6duxYtNsFACNENdxbWlq0fft2XXPNNRH7q6qqtGXLFtXU1KilpUU+n09z5sxRX1+fXRMIBFRfX6+6ujo1Nzerv79fCxYs0NDQUDRbBgAjRC3c+/v7tWzZMj366KOaNGmSvd+yLFVXV2vTpk1asmSJsrOztXv3bn344Yeqra2VJIVCIe3YsUMPPvigioqKNGPGDO3du1dHjhzR/v37z/h54XBYvb29ERsAjFdRC/c1a9Zo/vz5Kioqitjf0dGhYDCo4uJie5/b7dasWbN08OBBSVJra6tOnDgRUeP3+5WdnW3XnK6yslIej8feMjIyonBUADA2RCXc6+rq9OKLL6qysnLYc8FgUJLk9Xoj9nu9Xvu5YDCoxMTEiIn/9JrTlZWVKRQK2VtnZ6cThwIAY5Ljtx/o7OzUnXfeqYaGBk2YMGHEOpfLFfHYsqxh+053thq32y23233uDQOAgRyf3FtbW9Xd3a3c3FzFx8crPj5eTU1N+tWvfqX4+Hh7Yj99Au/u7raf8/l8GhwcVE9Pz4g1AICROR7uhYWFOnLkiNra2uxt5syZWrZsmdra2nTZZZfJ5/OpsbHRfs3g4KCamppUUFAgScrNzVVCQkJETVdXl44ePWrXAABG5viyTEpKirKzsyP2JScnKz093d4fCARUUVGhrKwsZWVlqaKiQhMnTtTSpUslSR6PRytXrtT69euVnp6utLQ0bdiwQTk5OcO+oAUADBeTW/5u3LhRAwMDWr16tXp6epSXl6eGhgalpKTYNVu3blV8fLxKSko0MDCgwsJC7dq1S3FxcbFoGQDGFJdlWVasm4iG3t5eeTwehUIhpaamxrodG/cCx9sPzI91C7iARvpv/nz+PTiXXOPeMgBgIMIdAAxEuAOAgQh3ADAQ4Q4ABiLcAcBAhDsAGIhwBwADEe4AYCDCHQAMRLgDgIEIdwAwUEzuCgmMZ07eSAoYCZM7ABiIcAcAA7EsA4wSLNfASUzuAGAgwh0ADES4A4CBCHcAMBDhDgAGItwBwECEOwAYiHAHAAMR7gBgIK5QBRCBK2XNwOQOAAYi3AHAQIQ7ABiIcAcAAxHuAGAgwh0ADES4A4CBCHcAMBAXMUXJSBeCAMCFQLgD4xQDiNlYlgEAAzG5A+MAU/r4w+QOAAYi3AHAQCzLAAZh+QUfY3IHAAMxuQNjEBM6PgmTOwAYiHAHAAOxLAOMcizB4Hw4PrlXVlbq+uuvV0pKiqZMmaLFixfrtddei6ixLEvl5eXy+/1KSkrS7Nmz1d7eHlETDodVWlqqyZMnKzk5WYsWLdKxY8ecbhcAjOR4uDc1NWnNmjU6dOiQGhsb9dFHH6m4uFjHjx+3a6qqqrRlyxbV1NSopaVFPp9Pc+bMUV9fn10TCARUX1+vuro6NTc3q7+/XwsWLNDQ0JDTLQOAcRxfltm3b1/E4507d2rKlClqbW3VTTfdJMuyVF1drU2bNmnJkiWSpN27d8vr9aq2tlarVq1SKBTSjh07tGfPHhUVFUmS9u7dq4yMDO3fv19z584d9rnhcFjhcNh+3Nvb6/ShAcCYEfUvVEOhkCQpLS1NktTR0aFgMKji4mK7xu12a9asWTp48KAkqbW1VSdOnIio8fv9ys7OtmtOV1lZKY/HY28ZGRnROiQAGPWiGu6WZWndunW68cYblZ2dLUkKBoOSJK/XG1Hr9Xrt54LBoBITEzVp0qQRa05XVlamUChkb52dnU4fDgCMGVE9W2bt2rV6+eWX1dzcPOw5l8sV8diyrGH7Tne2GrfbLbfbff7NAjirM5218/YD82PQCT6NqE3upaWlevrpp/Xcc89p2rRp9n6fzydJwybw7u5ue5r3+XwaHBxUT0/PiDUAgJE5Hu6WZWnt2rV64okn9OyzzyozMzPi+czMTPl8PjU2Ntr7BgcH1dTUpIKCAklSbm6uEhISImq6urp09OhRuwYAMDLHl2XWrFmj2tpaPfXUU0pJSbEndI/Ho6SkJLlcLgUCAVVUVCgrK0tZWVmqqKjQxIkTtXTpUrt25cqVWr9+vdLT05WWlqYNGzYoJyfHPnsGADAyx8N927ZtkqTZs2dH7N+5c6e+//3vS5I2btyogYEBrV69Wj09PcrLy1NDQ4NSUlLs+q1btyo+Pl4lJSUaGBhQYWGhdu3apbi4OKdbBgDjuCzLsmLdRDT09vbK4/EoFAopNTX1gn8+l4xjPOAL1U82Uhacz5/dueQaNw4DAAMR7gBgIMIdAAxEuAOAgbifO4Dz5uSXhXAWkzsAGIhwBwADsSwDwHHcZCz2mNwBwECEOwAYiHAHAAOx5g4gpjidMjqY3AHAQIQ7ABiIcAcAAxHuAGAgvlAFcEHwAzYXFpM7ABiIcAcAA7EsA2BcGS/n1RPuAEYlbj722RDuAMaM8TJ1O4E1dwAwEOEOAAZiWQaAscbzufVM7gBgICZ3AGPeeJ7QR8LkDgAGItwBwECEOwAYiDV3AJB5F0gxuQOAgZjcHcA39QBGGyZ3ADAQ4Q4ABmJZBgDO4lyWXUfTl6+EOwA4ZDR9/8ayDAAYiHAHAAMR7gBgIMIdAAxEuAOAgQh3ADAQ4Q4ABiLcAcBAXMR0DkbTBQoAcDajfnJ/+OGHlZmZqQkTJig3N1d//etfY90SAIx6o3pyf/zxxxUIBPTwww/rhhtu0COPPKJ58+bplVde0fTp06P2uUzoAMY6l2VZVqybGEleXp6uu+46bdu2zd535ZVXavHixaqsrIyoDYfDCofD9uNQKKTp06ers7NTqamp5/S52ff86bM1DgCf4Oj/m3vOr+nt7VVGRoY++OADeTyesxdbo1Q4HLbi4uKsJ554ImL/HXfcYd10003D6u+55x5LEhsbG5vxW2dn5ydm6Khdlvn3v/+toaEheb3eiP1er1fBYHBYfVlZmdatW2c/PnnypP7zn/8oPT1dLpfL3v/x//nOZ6IfSzhOs3CcZjnf47QsS319ffL7/Z9YO2rD/WP/fzBLpw7u9H2S5Ha75Xa7I/ZddNFFI75vamqq0f/yfIzjNAvHaZbzOc5PXI75P6P2bJnJkycrLi5u2JTe3d09bJoHAEQateGemJio3NxcNTY2RuxvbGxUQUFBjLoCgLFhVC/LrFu3TsuXL9fMmTOVn5+v7du3691339Xtt99+3u/pdrt1zz33DFvCMQ3HaRaO0ywX4jhH9amQ0qmLmKqqqtTV1aXs7Gxt3bpVN910U6zbAoBRbdSHOwDg3I3aNXcAwPkj3AHAQIQ7ABiIcAcAA42rcB8Ptw8+cOCAFi5cKL/fL5fLpSeffDLWLTmusrJS119/vVJSUjRlyhQtXrxYr732Wqzbiopt27bpmmuusa9kzM/P1zPPPBPrtqKqsrJSLpdLgUAg1q04rry8XC6XK2Lz+XxR+axxE+4f3z5406ZNeumll/S1r31N8+bN07vvvhvr1hx1/PhxXXvttaqpqYl1K1HT1NSkNWvW6NChQ2psbNRHH32k4uJiHT9+PNatOW7atGl64IEHdPjwYR0+fFjf+MY3dPPNN6u9vT3WrUVFS0uLtm/frmuuuSbWrUTN1Vdfra6uLns7cuRIdD7IgRs4jglf+cpXrNtvvz1i3xVXXGHdfffdMeoo+iRZ9fX1sW4j6rq7uy1JVlNTU6xbuSAmTZpk/fa3v411G47r6+uzsrKyrMbGRmvWrFnWnXfeGeuWHHfPPfdY11577QX5rHExuQ8ODqq1tVXFxcUR+4uLi3Xw4MEYdQWnhEIhSVJaWlqMO4muoaEh1dXV6fjx48rPz491O45bs2aN5s+fr6Kioli3ElVvvPGG/H6/MjMzdeutt+qtt96KyueM6tsPOOVcbx+MscOyLK1bt0433nijsrOzY91OVBw5ckT5+fn673//q89//vOqr6/XVVddFeu2HFVXV6cXX3xRLS0tsW4lqvLy8vTYY4/pS1/6kt5//33dd999KigoUHt7u9LT0x39rHER7h/7tLcPxtixdu1avfzyy2pubo51K1Fz+eWXq62tTR988IH+8Ic/aMWKFWpqajIm4Ds7O3XnnXeqoaFBEyZMiHU7UTVv3jz7n3NycpSfn68vfOEL2r17d8TvUThhXIQ7tw82U2lpqZ5++mkdOHBA06ZNi3U7UZOYmKgvfvGLkqSZM2eqpaVFv/zlL/XII4/EuDNntLa2qru7W7m5ufa+oaEhHThwQDU1NQqHw4qLi4thh9GTnJysnJwcvfHGG46/97hYc+f2wWaxLEtr167VE088oWeffVaZmZmxbumCsiwr4veCx7rCwkIdOXJEbW1t9jZz5kwtW7ZMbW1txga7dOq3n1999VVNnTrV8fceF5O7FJ3bB49G/f39evPNN+3HHR0damtrU1pamqZPnx7DzpyzZs0a1dbW6qmnnlJKSor9NzKPx6OkpKQYd+esn/zkJ5o3b54yMjLU19enuro6/eUvf9G+ffti3ZpjUlJShn1fkpycrPT0dOO+R9mwYYMWLlyo6dOnq7u7W/fdd596e3u1YsUK5z/sgpyTM0r8+te/ti655BIrMTHRuu6664w8de6555474w/qrlixItatOeZMxyfJ2rlzZ6xbc9xtt91m/zt78cUXW4WFhVZDQ0Os24o6U0+FvOWWW6ypU6daCQkJlt/vt5YsWWK1t7dH5bO45S8AGGhcrLkDwHhDuAOAgQh3ADAQ4Q4ABiLcAcBAhDsAGIhwBwADEe4AYCDCHQAMRLgDgIEIdwAw0P8CCQqtAZ/NEjgAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(4,4))\n",
    "plt.hist(y, bins=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is your task:\n",
    "\n",
    "* Compute the MAE of polynomial basis regression as a function of the degree of the monomials for the Boston housing dataset.  Start with degree equal to 1, i.e. using the features as-is, and compute the error for monomials with degrees 1 to 6.  In each case compute error for both the training set, and a separate test set.  Report the results as a nicely formatted table and comment on what you observe:  is there overfitting or underfitting?  We recommend using the scikit-learn [PolynomialFeatures](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html) functionality in conjunction with the [pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) class as shown in class.  Standardize the data for this task by including standardization as part of the pipeline.  Standardization should be performed before computing the monomials.\n",
    "* For degree four monomials your model should exhibit overfitting.  Our next step is to see whether we can improve the results by employing regularized linear regression, ie. ridge regression.  Plot the error of ridge regression using degree four monomial data as a function of the regularization parameter $\\alpha$ for both the training set and test set.  Is there a value of $\\alpha$ for which you are able to obtain test set error that is similar or better than the error of linear regression on the original data?  Comment on your plot, and describe where you are observing overfitting and underfitting.\n",
    "* As a baseline, compute the error of a naive regression method that for a given regression problem always returns the mean label value.  Compare this error with the error you got using the best performing model and comment on the result.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "boston = load_boston()\n",
    "X, y = boston.data, boston.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.3, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.fit_transform(X_test)\n",
    "\n",
    "degrees = range(1, 7)\n",
    "maes = []\n",
    "\n",
    "for d in degrees:\n",
    "  model = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('poly', PolynomialFeatures(degree=d, include_bias=False)),\n",
    "    ('ridge', Ridge(alpha=0.00001)) \n",
    "  ]) \n",
    "\n",
    "  model.fit(X_train, y_train) \n",
    "  y_pred = model.predict(X_test) \n",
    "\n",
    "  mae = mean_absolute_error(y_test, y_pred) \n",
    "  maes.append(mae)\n",
    "  print(f'degree {d}: mae {mae:.4f}') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3:  Use of AI and web resources\n",
    "\n",
    "In the cell below indicate in detail how you used AI and other web resources for this assignment.  If you used AI tools, indicate how useful they were."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I looked up the documentation for both `ridge_regression` and `Ridge`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Report\n",
    "\n",
    "Answer the questions in the cells reserved for that purpose.\n",
    "\n",
    "### Submission\n",
    "\n",
    "Submit your report as a Jupyter notebook via Canvas.  Running the notebook should generate all the plots in your notebook.\n",
    "\n",
    "### Grading \n",
    "\n",
    "\n",
    "```\n",
    "Grading sheet for assignment 4\n",
    "\n",
    "Part 1:  50 points.\n",
    "Dataset creation:  all the features were correctly converted to \n",
    "numerical values and the BMI and age values were standardized (15 pts)\n",
    "Evaluation of linear regression (15 pts)\n",
    "Feature selection based on weight vector magnitude, including discussion of the choice of features (15 pts)\n",
    "Comparison to the basline model and explanation why it is important (5 pts)\n",
    "\n",
    "Part 2:  50 points.\n",
    "Train/test accuracy as a function of the degree of polynomial \n",
    "regression and discussion of results (20 pts)\n",
    "Successful application of ridge regression to prevent overfitting and discussion of results (25 pts)\n",
    "Comparison to the basline model (5 pts)\n",
    "```\n",
    "\n",
    "Grading will be based on the following criteria:\n",
    "\n",
    "  * Code correctness.\n",
    "  * Code is well organized.\n",
    "  * Plots and other results are well formatted and easy to understand.\n",
    "  * Interesting and meaningful observations made where requested."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
